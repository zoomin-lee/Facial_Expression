{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UnobtrusivePain-torch-test.ipynb의 사본","provenance":[{"file_id":"14gBPfaChQRqdXVdg5f_vACwgiqXPNqjj","timestamp":1636610080696},{"file_id":"1ViT6t_FDcxwcUk8REEYey19aRzskzUmC","timestamp":1633967423183}],"collapsed_sections":["UiZmsNmpGEoB","vuSp1k7qG7yF"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vyNIbSdek2AL"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"6WY1n145NhuK"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJU5Ga54wvc3","cellView":"form","executionInfo":{"status":"ok","timestamp":1634057491603,"user_tz":-540,"elapsed":134021,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"554566a6-0cc1-462d-c52c-7cc1c81e6647"},"source":["#@markdown #Pytorch & pain_detection_demo.git\n","\n","!pip3 -q install torch==1.6 torchvision==0.7\n","!git clone https://github.com/TaatiTeam/pain_detection_demo.git &> /dev/null"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 748.8 MB 18 kB/s \n","\u001b[K     |████████████████████████████████| 5.9 MB 47.0 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","metadata":{"id":"UIgHWQR_HS3n"},"source":["## Scripts"]},{"cell_type":"markdown","metadata":{"id":"UiZmsNmpGEoB"},"source":["### pspi_au_detector.py"]},{"cell_type":"code","metadata":{"id":"anlw-gzjGGLM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634057644580,"user_tz":-540,"elapsed":536,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"54618eea-014c-4ce0-9e3c-a7624ce9e61d"},"source":["%%writefile /content/pain_detection_demo/pspi_au_detector.py\n","\"\"\"\n","    source:\n","        pain_detection_demo/pain_dector.py\n","    edit:\n","        * def predict_apin\n","            * pspi_predictions = predictions[0, -1]\n","        + def predict_aus\n","        + def predict_aus_and_pspi\n","\"\"\"\n","\n","import sys\n","sys.path.append('..')\n","sys.path.append('.')\n","import torch\n","import face_alignment as FAN\n","from models.comparative_model import ConvNetOrdinalLateFusion\n","import numpy as np\n","import cv2\n","from skimage.transform import SimilarityTransform, PiecewiseAffineTransform, warp\n","\n","\n","class PainDetector:\n","    def __init__(self, num_outputs, checkpoint_path='', fan_checkpoint='', image_size=160):\n","        \"\"\"\n","        :param checkpoint_path: model checkpoint path, cannot be empty\n","        :param fan_checkpoint: FAN checkpoint path, if empty will download pretrained model\n","        :param image_size: image size after face detection, must correspond to afar_checkpoint\n","        \"\"\"\n","        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","        self.ref_frames = []\n","        self.image_size = image_size\n","        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Histogram normalizer\n","        # load FAN landmark detector including SFD face detector\n","        self.FAN = FAN.FaceAlignment(FAN.LandmarksType._2D, flip_input=True, device=self.device,\n","                                     check_point_path=fan_checkpoint)\n","        self.face_detector = self.FAN.get_landmarks_from_image\n","        self.mean_lmks = np.load('standard_face_68.npy')\n","        self.mean_lmks = self.mean_lmks * 155 / self.mean_lmks.max()\n","        self.mean_lmks[:, 1] += 15\n","        # load model model\n","        self.model = ConvNetOrdinalLateFusion(num_outputs=num_outputs)\n","        self.model.load_state_dict(torch.load(checkpoint_path, map_location=self.device))\n","        self.model.eval()\n","\n","    @staticmethod\n","    def crop_image(frame, bbox):\n","        fh, fw = frame.shape[:2]\n","        bl, bt, br, bb = bbox\n","        fh, fw, bl, bt, br, bb = int(fh), int(fw), int(bl), int(bt), int(br), int(bb)\n","\n","        a_slice = frame[max(0, min(bt, fh)):min(fh, max(bb, 0)), max(0, min(bl, fw)):min(fw, max(br, 0)), :]\n","        new_image = np.zeros((bb - bt, br - bl, 3), dtype=np.float32)\n","        new_image[max(0, min(bt, fh)) - bt:min(fh, max(bb, 0)) - bt, max(0, min(bl, fw)) - bl:min(fw, max(br, 0)) - bl,\n","                  :] = a_slice\n","\n","        h, w = new_image.shape[:2]\n","        m = max(h, w)\n","        square_image = np.zeros((m, m, 3), dtype=np.float32)\n","        square_image[(m - h) // 2:h + (m - h) // 2, (m - w) // 2:w + (m - w) // 2, :] = new_image\n","        return square_image\n","\n","    @staticmethod\n","    def similarity_transform(image, landmarks):\n","        # anchor coordinate are based on the 240x320 resolution and need to be scaled accordingly for different size images.\n","        anchor_scale = 320 / image.shape[1]\n","        anchor = np.array([[110, 71], [210, 71], [160, 170]], np.float32) / anchor_scale\n","        idx = [36, 45, 57]\n","        tform = SimilarityTransform()\n","        tform.estimate(landmarks[idx, :], anchor)\n","        sim_mat = tform.params[:2, :]\n","        dst = cv2.warpAffine(image, sim_mat, (image.shape[1], image.shape[0]))\n","        dst_lmks = np.matmul(np.concatenate((landmarks, np.ones((landmarks.shape[0], 1))), 1), sim_mat.T)[:, :2]\n","        return dst, dst_lmks\n","\n","    @staticmethod\n","    def warp_lmks(tform, coords):\n","        out = np.empty_like(coords, np.double)\n","        # determine triangle index for each coordinate\n","        simplex = tform._inverse_tesselation.find_simplex(coords, tol=None)\n","        if (simplex == -1).any():  # simplex==-1 when point falls out of convex hull\n","            pass  # don;t know what to do yet\n","        for index in range(len(tform._inverse_tesselation.vertices)):\n","            # affine transform for triangle\n","            affine = tform.inverse_affines[index]\n","            # all coordinates within triangle\n","            index_mask = simplex == index\n","            out[index_mask, :] = affine(coords[index_mask, :])\n","        return out\n","\n","    @staticmethod\n","    def piecewise_affine_transform(image, source_lmks, target_lmks):\n","        anchor = list(range(31)) + [36, 39, 42, 45, 48, 51, 54, 57]\n","        tgt_lmks = target_lmks[anchor, :]\n","        dst_lmks = source_lmks[anchor, :]\n","        tform = PiecewiseAffineTransform()\n","        tform.estimate(tgt_lmks, dst_lmks)\n","        dst = warp(image, tform, output_shape=image.shape[:2]).astype(np.float32)\n","        return dst\n","\n","    def add_references(self, image_list):\n","        \"\"\"\n","        Use this to add a reference image for the model to compare target frames against.\n","        Reference images are assumed to have a PSPI score of zero\n","        :param image_list:  A list of numpy images of shape (H, W, 3)\n","        :return: None\n","        \"\"\"\n","        for image in image_list:\n","            self.ref_frames.append(self.prep_image(image))\n","\n","    def verify_refenerece_image(self, image, scale_to=320, color=(0, 255, 0), size=4, offset=(0, 0)):\n","        \"\"\"\n","        It run the input image through the pre-processing steps (most importantly face and landmark detection).\n","        It then draws the landmarks on the image so it can be visually inspected.\n","        :param image: An image with a frontal face in it\n","        :return: The input image with facial landmarks overlaid\n","        \"\"\"\n","        # Scaling the image to reduce its width to `scale_to`.\n","        # This makes sure that the run time is consistent by making sure the input image size is fixed.\n","        image = np.flip(image, axis=2)\n","        image = cv2.resize(image, (scale_to, int(image.shape[0] * scale_to/image.shape[1])), interpolation=cv2.INTER_AREA)\n","        landmarks = self.face_detector(image)\n","        num_faces = 0 if landmarks is None else len(landmarks)\n","        cv2.putText(image, '{} face/s detected'.format(num_faces), (7, 16), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (70, 70, 70))\n","        cv2.putText(image, '{} face/s detected'.format(num_faces), (6, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color)\n","        for lmk in landmarks:\n","            for lm in lmk.astype(np.int):\n","                cv2.drawMarker(image, (lm[0] + offset[0], lm[1] + offset[1]), color, cv2.MARKER_CROSS, size)\n","\n","        return image\n","\n","    def prep_image(self, image, scale_to=320):\n","        \"\"\"\n","        Runs images through the preprocessing steps\n","        :param image: A numpy image of shape (H, W, 3). The image should only have one face in it\n","        :return: Returns an image ready to be passed to the model\n","        \"\"\"\n","        # Scaling the image to reduce its width to `scale_to`.\n","        # This makes sure that the run time is consistent by making sure the input image size is fixed.\n","        image = cv2.resize(image, (scale_to, int(image.shape[0] * scale_to/image.shape[1])), interpolation=cv2.INTER_AREA)\n","        # We need to `mean_lmks`, because `self.mean_lmks` is based on 240x320 resolution images\n","        mean_lmks = self.mean_lmks * scale_to / 320\n","        landmarks = self.face_detector(image)\n","        if len(landmarks) > 1:\n","            ValueError('Reference image had more than one face. I should only have one')\n","        else:\n","            landmark = landmarks[0]\n","        image_face, lmks = self.similarity_transform(image, landmark)\n","        image_face = self.piecewise_affine_transform(image_face, lmks, mean_lmks)\n","        landmark = mean_lmks.round().astype(np.int)\n","        b_box = [landmark[:, 0].min(), landmark[:, 1].min(), landmark[:, 0].max(), landmark[:, 1].max()]\n","        image_face = self.crop_image(image_face, b_box)\n","        image_face = cv2.resize(image_face, (self.image_size, self.image_size))\n","        if len(image_face.shape) > 2 and image_face.shape[2] == 3:\n","            image_face = np.matmul(image_face, np.array([[0.114], [0.587], [0.299]]))\n","        image_face = self.clahe.apply((image_face * 255).astype(np.uint8))\n","        image_face = image_face.reshape(1, 1, self.image_size, self.image_size).astype(np.float32)\n","        return torch.from_numpy(image_face) / 255\n","\n","    def predict_pain(self, image):\n","        \"\"\"\n","        Main predictor function, takes an image as input and returns a float number as pain prediction\n","        :param image: RGB input image, size (Height x Width x Channel)\n","        :return: a float32 number, the estimated pain score\n","        \"\"\"\n","        pain_scores = []\n","        target_frame = self.prep_image(image)\n","        with torch.no_grad():\n","            # Gets a prediction for the target frame using every reference frame.\n","            for ref_frame in self.ref_frames:\n","                frames = torch.cat([target_frame, ref_frame], dim=1)\n","                predictions = self.model(frames).detach().cpu().numpy()\n","                # pspi_predictions = predictions[0, -3:]  # The last 3 outputs predict PSPI for Dementia, Healthy, and UNBC.\n","                pspi_predictions = predictions[0, -1]  # UNBC only\n","                pspi_predictions = np.clip(pspi_predictions, 0, None)  # Because PSPI >= 0\n","                pain_scores.append(pspi_predictions)\n","        # computes the mean of all the predictions.\n","        mean_score = np.array(pain_scores).mean()\n","        return mean_score\n","\n","    def predict_aus(self, image):\n","        aus = []\n","        target_frame = self.prep_image(image)\n","        with torch.no_grad():\n","            # Gets a prediction for the target frame using every reference frame.\n","            for ref_frame in self.ref_frames:\n","                frames = torch.cat([target_frame, ref_frame], dim=1)\n","                predictions = self.model(frames).detach().cpu().numpy()\n","                # au_predictions = predictions[0, :-3]\n","                au_predictions = predictions[0, :-1]\n","                au_predictions = np.clip(au_predictions, 0, None)\n","                aus.append(au_predictions)\n","        mean_aus = np.array(aus).mean(axis=0)\n","        return mean_aus\n","\n","    def predict_aus_and_pspi(self, image):\n","        aus = []\n","        pain_scores = []\n","\n","        target_frame = self.prep_image(image)\n","        with torch.no_grad():\n","            # Gets a prediction for the target frame using every reference frame.\n","            for ref_frame in self.ref_frames:\n","                frames = torch.cat([target_frame, ref_frame], dim=1)\n","                predictions = self.model(frames).detach().cpu().numpy()\n","\n","                au_predictions = predictions[0, :-1]\n","                pspi_predictions = predictions[0, -1]  # UNBC only\n","\n","                au_predictions = np.clip(au_predictions, 0, None)\n","                pspi_predictions = np.clip(pspi_predictions, 0, None)  # Because PSPI >= 0\n","\n","                aus.append(au_predictions)\n","                pain_scores.append(pspi_predictions)\n","\n","        mean_aus = np.array(aus).mean(axis=0)\n","        mean_score = np.array(pain_scores).mean()\n","        return mean_aus, mean_score"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/pain_detection_demo/pspi_au_detector.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"C9plypjYGeJL"},"source":["### test_pspi_au.py"]},{"cell_type":"code","metadata":{"id":"sMjyyT8IGfaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634057644581,"user_tz":-540,"elapsed":10,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"d67febac-db7e-4dec-d666-43063b128bc8"},"source":["%%writefile /content/pain_detection_demo/test_pspi_au.py\n","\"\"\"\n","    source:\n","        pain_detection_demo/test.py\n","    edit:\n","        * pain_detector.py -> pspi_au_detector.py\n","\"\"\"\n","\n","# from pain_detector import PainDetector\n","from pspi_au_detector import PainDetector\n","import cv2\n","import time\n","import argparse\n","import os\n","import glob\n","import pandas as pd\n","\n","\n","parser = argparse.ArgumentParser(description='Trains!')\n","parser.add_argument('-unbc_only', action='store_true', default=False,\n","                    help='Load the checkpoint that was only trained on UNBC. Otherwise loaded the checkpoint that was train on Both UNBC and UofR datasets')\n","parser.add_argument('-test_framerate', action='store_true', default=False,\n","                    help='Runs frame rate test as well')\n","parser.add_argument('-v', action='store_true', default=False, help='verbosity')\n","parser.add_argument('-test_data', type=str, required=True, help='Path to test images directory')\n","parser.add_argument('--s', type=str, help='Path to result csv')\n","\n","args = parser.parse_args()\n","\n","if args.unbc_only:\n","    pain_detector = PainDetector(image_size=160,\n","                                 checkpoint_path='checkpoints/59448122/59448122_3/model_epoch13.pt',\n","                                 num_outputs=7)\n","else:\n","    pain_detector = PainDetector(image_size=160,\n","                                 checkpoint_path='checkpoints/50342566/50343918_3/model_epoch4.pt',\n","                                 num_outputs=40)\n","\n","print('Device: ', pain_detector.device)\n","\n","test_data = args.test_data\n","ref_frames_dir = os.path.join(test_data, 'reference_frames')\n","target_frames_dir = os.path.join(test_data, 'target_frames')\n","exts = ['*.jpg', '*.jpeg', '*.png']\n","\n","ref_frames = []\n","for e in exts:\n","    ref_frames.extend(glob.glob(os.path.join(ref_frames_dir, e)))\n","ref_frames = sorted(ref_frames)\n","\n","ref_frame_1 = cv2.imread(ref_frames[0])\n","ref_frame_2 = cv2.imread(ref_frames[1])\n","ref_frame_3 = cv2.imread(ref_frames[2])\n","\n","pain_detector.add_references([ref_frame_1, ref_frame_2, ref_frame_3])\n","\n","target_frames = []\n","for e in exts:\n","    target_frames.extend(glob.glob(os.path.join(target_frames_dir, e)))\n","target_frames = sorted(target_frames)\n","\n","au_estimate_all = []\n","au_to_pspi_all = []\n","pain_estimate_all = []\n","\n","for t in target_frames:\n","    target_frame = cv2.imread(t)\n","    target_frame_name = os.path.basename(t)\n","    au_estimate, pain_estimate = pain_detector.predict_aus_and_pspi(target_frame)\n","    \n","    au_4, au_6, au_7, au_9, au_10, au_43 = au_estimate  # TODO: check order\n","    au_43_bin = 0 if round(au_43)==0 else 1\n","    au_to_pspi = au_4 + max(au_6, au_7) + max(au_9, au_10) + au_43_bin\n","\n","    au_estimate_all.append(au_estimate)\n","    pain_estimate_all.append(pain_estimate)\n","    au_to_pspi_all.append(au_to_pspi)\n","\n","    if args.v:\n","        print(f'[{target_frame_name}] pspi: {pain_estimate:.2f}')\n","        print(\n","            f'[{target_frame_name}] AU04: {au_4:.2f} AU06: {au_6:.2f} AU07: {au_7:.2f}',\n","            f'AU09: {au_9:.2f} AU10: {au_10:.2f} AU43: {au_43:.2f} => pspi {au_to_pspi:.2f}')\n","\n","if args.s is not None:\n","    df = pd.DataFrame(au_estimate_all, columns=['AU4', 'AU6', 'AU7', 'AU9', 'AU10', 'AU43'])\n","    df['PSPI_from_AU'] = au_to_pspi_all\n","    df['PSPI'] = pain_estimate_all\n","    df.insert(0, 'frame', [os.path.basename(t) for t in target_frames])\n","    df.to_csv(args.s, header=True)\n","\n","\n","if args.test_framerate:\n","    num_of_frames = 30\n","    print('Testing frame rate with {} frames'.format(num_of_frames))\n","    start_time = time.time()\n","    for _ in range(num_of_frames):\n","        pain_detector.predict_pain(target_frame)\n","    print('FPS: {}'.format(num_of_frames / (time.time() - start_time)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/pain_detection_demo/test_pspi_au.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"vuSp1k7qG7yF"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"NKNKWvZHG8iZ"},"source":["import cv2\n","import numpy as np\n","import csv\n","\n","def frames_to_video(frames_dir, output_path):\n","    filenames = [f for f in os.listdir(frames_dir) if \n","                 f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","    filenames = sorted(filenames)\n","    \n","    first_frame = cv2.imread(os.path.join(frames_dir, filenames[0]))\n","    h, w, _ = first_frame.shape\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","    video = cv2.VideoWriter(output_path, fourcc, 30, (w,h))\n","    \n","    for filename in filenames:\n","        video.write(cv2.imread(os.path.join(frames_dir, filename)))\n","\n","    video.release()\n","    cv2.destroyAllWindows()\n","\n","    print(f'Successfully generated! {output_path}')\n","\n","\n","def pspi_text_to_csv(pspi_true_dir, output_csv):\n","    filenames = [f for f in os.listdir(pspi_true_dir) if f.lower().endswith('.txt')]\n","    filenames = sorted(filenames)\n","\n","    frame_num = np.arange(len(filenames))\n","    pspi = []\n","    for filename in filenames:\n","        with open(os.path.join(pspi_true_dir, filename), 'r') as txt_file:\n","            p = txt_file.readline()\n","            pspi.append(p)\n","\n","    rows = zip(frame_num, pspi)\n","    with open(output_csv, 'w', newline='') as csv_file:\n","        writer = csv.writer(csv_file)\n","        writer.writerow(('frame_num', 'gt_pspi'))\n","        for row in rows:\n","            writer.writerow(row)\n","\n","    print(f'Ground Truth PSPI csv file saved to {output_csv}')\n","\n","\n","def aus_to_pspi(aus):\n","    \"\"\" Compute PSPI from AU intensities\n","        PSPI = AU_4 + max(AU_6,AU_7) + max(AU_9,AU_10) + AU_43\n","        \n","        Input:\n","            aus: a list of AU intensities\n","                * use AU_45 (blink) instead of AU_43 (eyes_closed)\n","                * list values in increasing order of the unit number\n","                  0:4, 1:6, 2:7, 3:9, 4:10, 5:45\n","        Output:\n","            PSPI score in float\n","    \"\"\"\n","    term_1 = aus[0]\n","    term_2 = max(aus[1], aus[2])\n","    term_3 = max(aus[3], aus[4])\n","    term_4 = 0 if round(aus[5])==0 else 1\n","\n","    pspi = term_1 + term_2 + term_3 + term_4\n","\n","    return round(pspi, 2)\n","\n","\n","def plot_pspi(pspi_true_csv, pspi_pred_csv, save_path, display=False):\n","    true_df = pd.read_csv(pspi_true_csv)\n","    pred_df = pd.read_csv(pspi_pred_csv)\n","    x = true_df['frame_num']\n","    y_true = true_df['gt_pspi']\n","    y_pred = pred_df['PSPI_from_AU']\n","\n","    plt.plot(x, y_true, label='Ground Truth')\n","    plt.plot(x, y_pred, label='PSPI from AU')\n","    plt.ylim((0,15))\n","    plt.yticks(np.arange(16))\n","    plt.xlabel('Frames')\n","    plt.ylabel('PSPI')\n","\n","    plt.legend(loc='upper right')\n","\n","    plt.savefig(save_path)\n","\n","    if display:\n","        plt.show()\n","    else:\n","        plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oy8ApypaGU7q"},"source":["# Data"]},{"cell_type":"code","metadata":{"cellView":"form","id":"yCxjgTtHNonx"},"source":["#@markdown #Load *UNBC-McMaster shoulder pain* Dataset\n","\n","!mkdir -p './dataset/UNBCMcMaster/'\n","!cp '/gdrive/MyDrive/datasets/UNBC-McMaster/Frame_Labels.zip' './dataset/UNBCMcMaster/'\n","!cp '/gdrive/MyDrive/datasets/UNBC-McMaster/Images.zip' './dataset/UNBCMcMaster/'\n","!unzip -q './dataset/UNBCMcMaster/Frame_Labels.zip' -d './dataset/UNBCMcMaster/'\n","!unzip -q './dataset/UNBCMcMaster/Images.zip' -d './dataset/UNBCMcMaster/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"EHtFErDeHuQd","executionInfo":{"status":"ok","timestamp":1634054971599,"user_tz":-540,"elapsed":690,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"1a250937-5b5b-48e7-db6e-8a3c935eee00"},"source":["import os\n","\n","dataset_dir = '/content/dataset/UNBCMcMaster/' #@param {type: 'string'}\n","patient_name = '064-ak064' #@param {type: 'string'}\n","sequence_name = 'ak064t1aaaff' #@param {type: 'string'}\n","\n","test_name = 'test-64-1' #@param {type: 'string'}\n","\n","img_dir = os.path.join(dataset_dir, 'Images', patient_name, sequence_name)\n","gt_pspi_dir = os.path.join(dataset_dir, 'Frame_Labels/PSPI', patient_name, sequence_name)\n","\n","test_data_dir = f'/content/pain_detection_demo/{test_name}'\n","test_data_ref = f'{test_data_dir}/reference_frames'\n","test_data_target = f'/content/pain_detection_demo/{test_name}/target_frames'\n","\n","!mkdir -p $test_data_ref\n","!mkdir -p $test_data_target\n","\n","ref_frame_1 = f'{img_dir}/{sequence_name}001.png'\n","ref_frame_2 = f'{img_dir}/{sequence_name}002.png'\n","ref_frame_3 = f'{img_dir}/{sequence_name}003.png'\n","\n","result_data_dir = f'/content/{test_name}_results'\n","!mkdir -p $result_data_dir\n","\n","result_csv = f'{result_data_dir}/{test_name}.csv'\n","gt_pspi_csv = f'/content/dataset/gt-pspi_{test_name}.csv'\n","patient_vid = f'{result_data_dir}/{test_name}.mp4'\n","au_vid = f'{result_data_dir}/{test_name}_au.mp4'\n","pspi_vid = f'{result_data_dir}/{test_name}_pspi.mp4'\n","output_vid = f'{result_data_dir}/{test_name}_out.mp4'\n","pspi_png = f'{result_data_dir}/{test_name}_pspi.png'\n","\n","print(f'         img_dir: {img_dir}')\n","print(f'     gt_pspi_dir: {gt_pspi_dir}')\n","print(f'   test_data_dir: {test_data_dir}')\n","print(f'   test_data_ref: {test_data_ref}')\n","print(f'test_data_target: {test_data_target}\\n')\n","\n","print(f'     ref_frame_1: {ref_frame_1}')\n","print(f'     ref_frame_2: {ref_frame_2}')\n","print(f'     ref_frame_3: {ref_frame_3}\\n')\n","\n","print(f' result_data_dir: {result_data_dir}')\n","print(f'      result_csv: {result_csv}')\n","print(f'     gt_pspi_csv: {gt_pspi_csv}')\n","print(f'     patient_vid: {patient_vid}')\n","print(f'          au_vid: {au_vid}')\n","print(f'        pspi_vid: {pspi_vid}')\n","print(f'      output_vid: {output_vid}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["         img_dir: /content/dataset/UNBCMcMaster/Images/064-ak064/ak064t1aaaff\n","     gt_pspi_dir: /content/dataset/UNBCMcMaster/Frame_Labels/PSPI/064-ak064/ak064t1aaaff\n","   test_data_dir: /content/pain_detection_demo/test-64-1\n","   test_data_ref: /content/pain_detection_demo/test-64-1/reference_frames\n","test_data_target: /content/pain_detection_demo/test-64-1/target_frames\n","\n","     ref_frame_1: /content/dataset/UNBCMcMaster/Images/064-ak064/ak064t1aaaff/ak064t1aaaff001.png\n","     ref_frame_2: /content/dataset/UNBCMcMaster/Images/064-ak064/ak064t1aaaff/ak064t1aaaff002.png\n","     ref_frame_3: /content/dataset/UNBCMcMaster/Images/064-ak064/ak064t1aaaff/ak064t1aaaff003.png\n","\n"," result_data_dir: /content/test-64-1_results\n","      result_csv: /content/test-64-1_results/test-64-1.csv\n","     gt_pspi_csv: /content/dataset/gt-pspi_test-64-1.csv\n","     patient_vid: /content/test-64-1_results/test-64-1.mp4\n","          au_vid: /content/test-64-1_results/test-64-1_au.mp4\n","        pspi_vid: /content/test-64-1_results/test-64-1_pspi.mp4\n","      output_vid: /content/test-64-1_results/test-64-1_out.mp4\n"]}]},{"cell_type":"code","metadata":{"id":"5giTpQOSJ1Ai"},"source":["# reference frames\n","!cp $ref_frame_1 $test_data_ref\n","!cp $ref_frame_2 $test_data_ref\n","!cp $ref_frame_3 $test_data_ref\n","\n","# target frames\n","!cp -r $img_dir/* $test_data_target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwdJt5brk9mc"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"Hv1VtIJ7-IPW"},"source":["## run pain detection"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7OJqgfNOX6a","executionInfo":{"status":"ok","timestamp":1634055083611,"user_tz":-540,"elapsed":107552,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"efbe88bb-b992-486a-cc18-0455e2468490"},"source":["%cd /content/pain_detection_demo\n","!python test_pspi_au.py --s $result_csv -unbc_only -test_data $test_data_dir"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pain_detection_demo\n","  is not a file. Loading model from:  /content/pain_detection_demo/pretrained\n","Device:  cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZNya9Y3--Liv"},"source":["## frames to video"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BOsOiBu-bn-","executionInfo":{"status":"ok","timestamp":1634055084811,"user_tz":-540,"elapsed":1223,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"9baff7e8-879a-4cbf-8100-79c83d432c98"},"source":["frames_to_video(test_data_target, patient_vid)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully generated! /content/test-64-1_results/test-64-1.mp4\n"]}]},{"cell_type":"markdown","metadata":{"id":"Sr5ldcBI-jiS"},"source":["## AU predictions to video"]},{"cell_type":"code","metadata":{"id":"HHF0lZwqYC-B"},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","fields = ['AU4', 'AU6', 'AU7', 'AU9', 'AU10', 'AU43']\n","\n","df = pd.read_csv(result_csv, usecols=fields)\n","\n","fig, ax = plt.subplots()\n","ax.set_ylim((0,5))\n","ax.set_xlabel('AU')\n","ax.set_ylabel('Intensity')\n","\n","bar_04 = df['AU4'][0]\n","bar_06 = df['AU6'][0]\n","bar_07 = df['AU7'][0]\n","bar_09 = df['AU9'][0]\n","bar_10 = df['AU10'][0]\n","bar_43 = df['AU43'][0]\n","    \n","bars = plt.bar(['4', '6', '7', '9', '10', '43'],\n","               [bar_04, bar_06, bar_07, bar_09, bar_10, bar_43], color='orange')\n","\n","def animate(i):\n","    h_04 = df['AU4'][i]\n","    h_06 = df['AU6'][i]\n","    h_07 = df['AU7'][i]\n","    h_09 = df['AU9'][i]\n","    h_10 = df['AU10'][i]\n","    h_43 = df['AU43'][i]\n","    h = [h_04, h_06, h_07, h_09, h_10, h_43]\n","    \n","    for j, bar in enumerate(bars):\n","        bar.set_height(h[j])\n","    ax.set_title(f'Frame #{i}')\n","\n","frames = len(df)\n","ani = FuncAnimation(fig, animate, blit=False, \n","                    frames=np.arange(frames),\n","                    interval=33,\n","                    save_count=frames,\n","                    )\n","\n","# %matplotlib inline\n","# HTML(ani.to_html5_video())\n","\n","ani.save(au_vid, writer=animation.FFMpegWriter(fps=30))\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E4_3TgQt-mXs"},"source":["## pspi predictions to csv & video"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZt8e1ghcYzW","executionInfo":{"status":"ok","timestamp":1634055098090,"user_tz":-540,"elapsed":17,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"b34b7034-f9bf-485e-a564-8fe0ca01c031"},"source":["pspi_text_to_csv(gt_pspi_dir, gt_pspi_csv)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ground Truth PSPI csv file saved to /content/dataset/gt-pspi_test-64-1.csv\n"]}]},{"cell_type":"code","metadata":{"id":"AWh7hgZ0dB1S"},"source":["plot_pspi(gt_pspi_csv, result_csv, pspi_png, display=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2twQWNZQdz99"},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","true_df = pd.read_csv(gt_pspi_csv)\n","df = pd.read_csv(result_csv, usecols=['PSPI', 'PSPI_from_AU'])\n","frames = len(df)\n","\n","fig, ax = plt.subplots()\n","ax.set_ylim((0,15))\n","ax.set_xlabel('Frame')\n","ax.set_ylabel('PSPI')\n","\n","x_true = true_df['frame_num']\n","y_true = true_df['gt_pspi']\n","\n","y_pred = df['PSPI']\n","y_from_au = df['PSPI_from_AU']\n","\n","plt.plot(x_true, y_true, label='Ground Truth')\n","plt.xlabel('Frames')\n","plt.ylabel('PSPI')\n","\n","line_1, = ax.plot(x_true, y_from_au, label='PSPI from AUs')\n","line_2, = ax.plot(x_true, y_pred, label='PSPI estimated')\n","\n","def update(i, x_true, y_from_au, y_pred, line_1, line_2):\n","    line_1.set_data(x_true[:i], y_from_au[:i])\n","    line_1.axes.axis([0, frames, 0, 15])\n","    line_1.set_label('PSPI from AU')\n","\n","    line_2.set_data(x_true[:i], y_pred[:i])\n","    line_2.axes.axis([0, frames, 0, 15])\n","    line_2.set_label('PSPI estimated')\n","\n","    plt.legend(loc='upper right')\n","    ax.set_title(f'from AUs: {round(y_from_au[i])}, estimated: {round(y_pred[i])}')\n","\n","    return line_1, line_2,\n","\n","\n","ani = FuncAnimation(fig, update,\n","                    fargs=[x_true, y_from_au, y_pred, line_1, line_2],\n","                    blit=False, \n","                    frames=np.arange(frames),\n","                    interval=33,\n","                    save_count=frames,\n","                    )\n","\n","# %matplotlib inline\n","# HTML(ani.to_html5_video())\n","\n","ani.save(pspi_vid, writer=animation.FFMpegWriter(fps=30))\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1XgG17-nIO4Y"},"source":["## patient + au + pspi to video"]},{"cell_type":"code","metadata":{"id":"IeA1dA8ZYh8a"},"source":["patient_vid_resized = f'{patient_vid[:-4]}-resized.mp4'\n","au_vid_resized = f'{au_vid[:-4]}-resized.mp4'\n","pspi_vid_resized = f'{pspi_vid[:-4]}-resized.mp4'\n","\n","# Resize patient video\n","!ffmpeg -n -i $patient_vid \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","-c:v libx264 $patient_vid_resized &> /dev/null\n","\n","# Resize AU histogram video\n","!ffmpeg -n -i $au_vid \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","    -c:v libx264 $au_vid_resized &> /dev/null\n","\n","# Resize PSPI histogram video\n","!ffmpeg -n -i $pspi_vid \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","    -c:v libx264 $pspi_vid_resized &> /dev/null\n","\n","# hstack patient video (left) & au plot video (right)\n","!ffmpeg -n -i $patient_vid_resized -i $au_vid_resized -i $pspi_vid_resized -filter_complex hstack=3 $output_vid &> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8US3hl23ILfj"},"source":["## Download Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmu8_7mado7e","executionInfo":{"status":"ok","timestamp":1634055125993,"user_tz":-540,"elapsed":35,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"47daf0e5-28c2-4cf6-c2f8-72e7c4342242"},"source":["result_zip = f'/content/{test_name}_results.zip'\n","!zip -r $result_zip $result_data_dir\n","\n","from google.colab import files\n","files.download(result_zip)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/test-64-1_results/ (stored 0%)\n","  adding: content/test-64-1_results/test-64-1_pspi.mp4 (deflated 21%)\n","  adding: content/test-64-1_results/test-64-1_pspi.png (deflated 4%)\n","  adding: content/test-64-1_results/test-64-1.mp4 (deflated 1%)\n","  adding: content/test-64-1_results/test-64-1.csv (deflated 70%)\n","  adding: content/test-64-1_results/test-64-1_au.mp4 (deflated 10%)\n","  adding: content/test-64-1_results/test-64-1_out.mp4 (deflated 2%)\n","  adding: content/test-64-1_results/test-64-1_au-resized.mp4 (deflated 12%)\n","  adding: content/test-64-1_results/test-64-1_pspi-resized.mp4 (deflated 23%)\n","  adding: content/test-64-1_results/test-64-1-resized.mp4 (deflated 2%)\n"]},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_4e6412fc-292a-41d3-90f1-f2d93e8f41d7\", \"test-64-1_results.zip\", 1998402)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"2JLng9xrYHVO"},"source":["# etc: youtube kid"]},{"cell_type":"code","metadata":{"id":"nuXmrgVlYudV"},"source":["def video_to_frames(video_dir, output_path):\n","    cap = cv2.VideoCapture(video_dir)\n","    i = 0\n","    while (cap.isOpened()):\n","        ret, frame = cap.read()\n","        if ret == False:\n","            break\n","        cv2.imwrite(f'{output_path}{i:04}.png', frame)\n","        i += 1\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    print(f'Successfully generated! {output_path}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"8POtYReEYJq3","executionInfo":{"status":"ok","timestamp":1634057663353,"user_tz":-540,"elapsed":13,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"d4740ef0-d281-4112-9a9a-5125f13180e8"},"source":["from IPython.display import YouTubeVideo\n","\n","# Change the Youtube_ID with the link to your group's video.\n","YOUTUBE_ID = 'YTKI-nDvKRo'\n","\n","YouTubeVideo(YOUTUBE_ID)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"400\"\n","            height=\"300\"\n","            src=\"https://www.youtube.com/embed/YTKI-nDvKRo\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.YouTubeVideo at 0x7f701ace0810>"],"image/jpeg":"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\n"},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaZI7Mb3YVZ-","executionInfo":{"status":"ok","timestamp":1634058691478,"user_tz":-540,"elapsed":926826,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"cb7efeeb-3f2e-421d-e0c2-6fcd0bad3dbc"},"source":["!pip install youtube_dl\n","!youtube-dl -f 'bestvideo[ext=mp4]' --output \"youtube.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: youtube_dl in /usr/local/lib/python3.7/dist-packages (2021.6.6)\n","[youtube] YTKI-nDvKRo: Downloading webpage\n","[download] Destination: youtube.mp4\n","\u001b[K[download] 100% of 39.10MiB in 15:22\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aaIY6OGvYYxl","executionInfo":{"status":"ok","timestamp":1634058736628,"user_tz":-540,"elapsed":30635,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"901d4680-c2fc-4398-c24e-6eee09b04fb8"},"source":["# trim video\n","!ffmpeg -y -loglevel info -i youtube.mp4 -ss 00:00:44 -t 10 /content/youtube_kid.mp4"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'youtube.mp4':\n","  Metadata:\n","    major_brand     : dash\n","    minor_version   : 0\n","    compatible_brands: iso6avc1mp41\n","    creation_time   : 2015-07-27T10:11:40.000000Z\n","  Duration: 00:02:11.56, start: 0.000000, bitrate: 2493 kb/s\n","    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 1920x1080 [SAR 1:1 DAR 16:9], 91 kb/s, 25 fps, 25 tbr, 90k tbn, 50 tbc (default)\n","    Metadata:\n","      creation_time   : 2015-07-27T10:11:40.000000Z\n","      handler_name    : VideoHandler\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n","Press [q] to stop, [?] for help\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0musing SAR=1/1\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mprofile High, level 4.0\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n","Output #0, mp4, to '/content/youtube_kid.mp4':\n","  Metadata:\n","    major_brand     : dash\n","    minor_version   : 0\n","    compatible_brands: iso6avc1mp41\n","    encoder         : Lavf57.83.100\n","    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 25 fps, 12800 tbn, 25 tbc (default)\n","    Metadata:\n","      creation_time   : 2015-07-27T10:11:40.000000Z\n","      handler_name    : VideoHandler\n","      encoder         : Lavc57.107.100 libx264\n","    Side data:\n","      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n","frame=  250 fps=8.5 q=-1.0 Lsize=    2648kB time=00:00:09.88 bitrate=2195.8kbits/s speed=0.335x    \n","video:2645kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.120728%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mframe I:2     Avg QP:18.66  size: 70696\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mframe P:82    Avg QP:19.13  size: 21779\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mframe B:166   Avg QP:22.12  size:  4702\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mconsecutive B-frames:  1.2%  0.8% 90.0%  8.0%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mmb I  I16..4: 10.1% 84.2%  5.7%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mmb P  I16..4:  4.1% 14.2%  0.2%  P16..4: 36.6%  7.7%  3.5%  0.0%  0.0%    skip:33.8%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mmb B  I16..4:  0.5%  0.9%  0.0%  B16..8: 30.7%  0.9%  0.0%  direct: 1.2%  skip:65.6%  L0:32.2% L1:66.3% BI: 1.5%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0m8x8 transform intra:76.0% inter:89.5%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mcoded y,uvDC,uvAC intra: 37.4% 41.7% 1.1% inter: 6.6% 10.8% 0.0%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mi16 v,h,dc,p: 33% 22% 28% 17%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 15% 45%  2%  3%  3%  2%  2%  2%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33% 16% 22%  4%  7%  7%  5%  4%  2%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mi8c dc,h,v,p: 55% 21% 23%  1%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mref P L0: 72.3%  9.1% 14.7%  3.9%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mref B L0: 80.7% 18.0%  1.4%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mref B L1: 99.5%  0.5%\n","\u001b[1;36m[libx264 @ 0x55681a495e00] \u001b[0mkb/s:2166.30\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCrNCxfVZrhj","executionInfo":{"status":"ok","timestamp":1634058763926,"user_tz":-540,"elapsed":18869,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"d3fc6296-1037-45f0-89c2-fa8dbe5fb7c6"},"source":["!mkdir -p /content/youtube_kid_frames/\n","video_to_frames('/content/youtube_kid.mp4', '/content/youtube_kid_frames/ytkid')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully generated! /content/youtube_kid_frames/ytkid\n"]}]},{"cell_type":"code","metadata":{"id":"7UE_BiMncHde"},"source":["!mkdir -p /content/yt_test_data/reference_frames\n","!mkdir -p /content/yt_test_data/target_frames\n","\n","# reference frames\n","!cp /content/youtube_kid_frames/ytkid0024.png /content/yt_test_data/reference_frames\n","!cp /content/youtube_kid_frames/ytkid0025.png /content/yt_test_data/reference_frames\n","!cp /content/youtube_kid_frames/ytkid0026.png /content/yt_test_data/reference_frames\n","\n","# target frames\n","!cp -r /content/youtube_kid_frames/* /content/yt_test_data/target_frames"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BHQ-_MtdFzb","executionInfo":{"status":"ok","timestamp":1634058997942,"user_tz":-540,"elapsed":78693,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"66cb5fb4-3e18-414c-b934-5e3e67c716cb"},"source":["%cd /content/pain_detection_demo\n","!python test_pspi_au.py --s /content/ytkid.csv -unbc_only -test_data /content/yt_test_data/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pain_detection_demo\n","  is not a file. Loading model from:  /content/pain_detection_demo/pretrained\n","Device:  cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"LdKzLyk-dmQl"},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","fields = ['AU4', 'AU6', 'AU7', 'AU9', 'AU10', 'AU43']\n","\n","df = pd.read_csv('/content/ytkid.csv', usecols=fields)\n","\n","fig, ax = plt.subplots()\n","ax.set_ylim((0,5))\n","ax.set_xlabel('AU')\n","ax.set_ylabel('Intensity')\n","\n","bar_04 = df['AU4'][0]\n","bar_06 = df['AU6'][0]\n","bar_07 = df['AU7'][0]\n","bar_09 = df['AU9'][0]\n","bar_10 = df['AU10'][0]\n","bar_43 = df['AU43'][0]\n","    \n","bars = plt.bar(['4', '6', '7', '9', '10', '43'],\n","               [bar_04, bar_06, bar_07, bar_09, bar_10, bar_43], color='orange')\n","\n","def animate(i):\n","    h_04 = df['AU4'][i]\n","    h_06 = df['AU6'][i]\n","    h_07 = df['AU7'][i]\n","    h_09 = df['AU9'][i]\n","    h_10 = df['AU10'][i]\n","    h_43 = df['AU43'][i]\n","    h = [h_04, h_06, h_07, h_09, h_10, h_43]\n","    \n","    for j, bar in enumerate(bars):\n","        bar.set_height(h[j])\n","    ax.set_title(f'Frame #{i}')\n","\n","frames = len(df)\n","ani = FuncAnimation(fig, animate, blit=False, \n","                    frames=np.arange(frames),\n","                    interval=33,\n","                    save_count=frames,\n","                    )\n","\n","# %matplotlib inline\n","# HTML(ani.to_html5_video())\n","\n","ani.save('/content/ytkid_au.mp4', writer=animation.FFMpegWriter(fps=30))\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0ULQJyfd3ED"},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","df = pd.read_csv('/content/ytkid.csv', usecols=['PSPI', 'PSPI_from_AU'])\n","frames = len(df)\n","\n","fig, ax = plt.subplots()\n","ax.set_ylim((0,15))\n","ax.set_xlabel('Frame')\n","ax.set_ylabel('PSPI')\n","\n","x = np.arange(frames)\n","y_pred = df['PSPI']\n","y_from_au = df['PSPI_from_AU']\n","\n","plt.xlabel('Frames')\n","plt.ylabel('PSPI')\n","\n","line_1, = ax.plot(x, y_from_au, label='PSPI from AUs')\n","line_2, = ax.plot(x, y_pred, label='PSPI estimated')\n","\n","def update(i, x, y_from_au, y_pred, line_1, line_2):\n","    line_1.set_data(x[:i], y_from_au[:i])\n","    line_1.axes.axis([0, frames, 0, 15])\n","    line_1.set_label('PSPI from AU')\n","\n","    line_2.set_data(x[:i], y_pred[:i])\n","    line_2.axes.axis([0, frames, 0, 15])\n","    line_2.set_label('PSPI estimated')\n","\n","    plt.legend(loc='upper right')\n","    ax.set_title(f'from AUs: {round(y_from_au[i])}, estimated: {round(y_pred[i])}')\n","\n","    return line_1, line_2,\n","\n","\n","ani = FuncAnimation(fig, update,\n","                    fargs=[x, y_from_au, y_pred, line_1, line_2],\n","                    blit=False, \n","                    frames=np.arange(frames),\n","                    interval=33,\n","                    save_count=frames,\n","                    )\n","\n","# %matplotlib inline\n","# HTML(ani.to_html5_video())\n","\n","ani.save('/content/ytkid_pspi.mp4', writer=animation.FFMpegWriter(fps=30))\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MopcSJSveYxT"},"source":["# Resize patient video\n","!ffmpeg -n -i '/content/youtube_kid.mp4' \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","-c:v libx264 '/content/youtube_kid-resized.mp4' &> /dev/null\n","\n","# Resize AU histogram video\n","!ffmpeg -n -i '/content/ytkid_au.mp4' \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","    -c:v libx264 '/content/ytkid_au-resized.mp4' &> /dev/null\n","\n","# Resize PSPI histogram video\n","!ffmpeg -n -i '/content/ytkid_pspi.mp4' \\\n","    -vf \"scale=w=320:h=240:force_original_aspect_ratio=1,pad=320:240:(ow-iw)/2:(oh-ih)/2\" \\\n","    -c:v libx264 '/content/ytkid_pspi-resized.mp4' &> /dev/null\n","\n","# hstack patient video (left) & au plot video (right)\n","!ffmpeg -n -i '/content/youtube_kid-resized.mp4' -i '/content/ytkid_au-resized.mp4' \\\n","     -i '/content/ytkid_pspi-resized.mp4' -filter_complex hstack=3 \\\n","     '/content/ytkid_out.mp4' &> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"MfZiiPP3e-8w","executionInfo":{"status":"ok","timestamp":1634059108919,"user_tz":-540,"elapsed":271,"user":{"displayName":"SH JEONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcBGxvO6U6X90KSdl5IFOMlubZLkZXxX_lQgP1=s64","userId":"16140698684073717785"}},"outputId":"8545bd00-a62f-43fb-e3d7-2cd5a9c7e64b"},"source":["from google.colab import files\n","files.download('/content/ytkid_out.mp4')\n","files.download('/content/ytkid.csv')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_9b29ebd4-e5ec-41a2-8961-d66a17cf5cef\", \"ytkid_out.mp4\", 120157)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_d37bf8f0-0746-4f4e-a2bf-c64490963c9b\", \"ytkid.csv\", 40253)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]}]}